{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6e43f9",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ebe9934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fad5224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda:0') #先調1再調0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0994207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfe6f063",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x=torch.randn(1000, 6724)\n",
    "x=x.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce8c68f",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1157f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Prepare data-1/2\n",
    "# DataLoader wraps a Dataset and provides minibatches, shuffling, multithreading, for you\n",
    "data_in=np.loadtxt('./data/preserved/input_div_U_2_1s.dat')\n",
    "data_out=np.loadtxt('./data/preserved/input_div_U_2_1s.dat')\n",
    "x_in=torch.Tensor(data_in)\n",
    "y_in=torch.Tensor(data_out)\n",
    "x_in=x_in.to(device)\n",
    "y_in=y_in.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "849bbccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 6400])\n",
      "torch.Size([1000, 6400])\n"
     ]
    }
   ],
   "source": [
    "x=x_in\n",
    "y=y_in\n",
    "print(x.size())\n",
    "print(y.size())\n",
    "loader=DataLoader(TensorDataset(x, y), batch_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782c5516",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee717c30",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.44 GiB (GPU 0; 3.00 GiB total capacity; 2.50 GiB already allocated; 0 bytes free; 2.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 77>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m### Set the random seed for reproducible results\u001b[39;00m\n\u001b[0;32m     76\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 77\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mVariationalAutoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:899\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    895\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    896\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m    897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m--> 899\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 570\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    573\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    574\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    575\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    581\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 570\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    573\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    574\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    575\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    581\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 570\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    573\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    574\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    575\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    581\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:593\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 593\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    594\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:897\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m    895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    896\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m--> 897\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.44 GiB (GPU 0; 3.00 GiB total capacity; 2.50 GiB already allocated; 0 bytes free; 2.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VariationalEncoder(nn.Module):\n",
    "    def __init__(self, latent_dims):  \n",
    "        super(VariationalEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, 3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(4, 8, 3, stride=1, padding=1)\n",
    "        self.batch2 = nn.BatchNorm2d(8)\n",
    "        self.conv3 = nn.Conv2d(8, 16, 3, stride=1, padding=1)  \n",
    "        self.linear1 = nn.Linear(80*80*16, 6400)\n",
    "        self.linear2 = nn.Linear(6400, latent_dims)\n",
    "        self.linear3 = nn.Linear(6400, latent_dims)\n",
    "\n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "        self.N.loc = self.N.loc.cuda() # hack to get sampling on the GPU\n",
    "        self.N.scale = self.N.scale.cuda()\n",
    "        self.kl = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.batch2(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        mu =  self.linear2(x)\n",
    "        sigma = torch.exp(self.linear3(x))\n",
    "        z = mu + sigma*self.N.sample(mu.shape)\n",
    "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n",
    "        return z      \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            nn.Linear(latent_dims, 6400),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(6400, 80 * 80 * 16),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(16, 80, 80))\n",
    "\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, 8, 3, stride=1, output_padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 4, 3, stride=1, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(4, 1, 3, stride=1, padding=1, output_padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.decoder_lin(x)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.decoder_conv(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.encoder = VariationalEncoder(latent_dims)\n",
    "        self.decoder = Decoder(latent_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "    \n",
    "### Set the random seed for reproducible results\n",
    "torch.manual_seed(0)\n",
    "model = VariationalAutoencoder(latent_dims=128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a92f9540",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self, channel_1, channel_2, channel_3, channel_4, channel_5, kernel_dim):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, channel_1, kernel_dim)\n",
    "        self.conv2 = nn.Conv2d(channel_1, channel_2, kernel_dim)\n",
    "        self.conv3 = nn.Conv2d(channel_2, channel_3, kernel_dim)\n",
    "        self.conv4 = nn.Conv2d(channel_3, channel_4, kernel_dim)\n",
    "        self.conv5 = nn.Conv2d(channel_4, channel_5, kernel_dim)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(70*70*3, 6400)  # 78*78 from image dimension\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 6400)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "model = ConvNet(channel_1=3, channel_2=3, channel_3=3, channel_4=3, channel_5=3, kernel_dim=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58d1db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.nn.Sequential(torch.nn.Unflatten(1, (1,80,80)))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ea993ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=DataLoader(TensorDataset(x, x), batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e42a038",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3229b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tol=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99f15f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.RMSprop(model.parameters(), lr=0.001, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "687717dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1500, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66d34a83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs    Loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ENCHOU\\AppData\\Local\\Temp\\ipykernel_24720\\719664147.py:25: UserWarning: Using a target size (torch.Size([100, 1, 80, 80])) that is different to the input size (torch.Size([100, 80, 80])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss=torch.nn.functional.mse_loss(L_y_pred, x_batch)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 360.00 MiB (GPU 0; 3.00 GiB total capacity; 1.85 GiB already allocated; 0 bytes free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Update Weights\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#print(\"1 batch\")\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[1;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\adam.py:133\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[0;32m    131\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 133\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m           \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m           \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\_functional.py:94\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     92\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(bias_correction2))\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 94\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias_correction2\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m     96\u001b[0m step_size \u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m/\u001b[39m bias_correction1\n\u001b[0;32m     98\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 360.00 MiB (GPU 0; 3.00 GiB total capacity; 1.85 GiB already allocated; 0 bytes free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "loss_epoch=[]\n",
    "loss_values = []\n",
    "loss=1\n",
    "epochs=0\n",
    "\n",
    "print(\"Epochs    Loss\")\n",
    "\n",
    "while(loss>tol):\n",
    "    epochs=epochs+1\n",
    "    #scheduler.step()\n",
    "    \n",
    "    for x_batch, y_batch in loader:\n",
    "        # Forward pass\n",
    "        y_pred=model(x_batch)\n",
    "        #x_2d= torch.nn.Sequential(torch.nn.Unflatten(1, (1,80,80)))(x_batch)\n",
    "        \n",
    "        # Compute laplacian\n",
    "        y_pred= torch.nn.Sequential(torch.nn.Unflatten(1, (80,80)))(y_pred)\n",
    "        z=torch.concat([y_pred[:, :, :1], y_pred], axis=2)[:, :, :80]\n",
    "        w=torch.concat([y_pred, y_pred[:, :, 79:]], axis=2)[:, :, 1:]\n",
    "        m=torch.concat([y_pred[:, :1, :], y_pred], axis=1)[:, :80, :]\n",
    "        n=torch.concat([y_pred, y_pred[:, 79:, :]], axis=1)[:, 1:, :]\n",
    "        L_y_pred=(z+w+m+n-4*y_pred)*40 # h=1/80 /2h=>*40\n",
    "        \n",
    "        loss=torch.nn.functional.mse_loss(L_y_pred, x_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.requires_grad_(True)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Weights\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        #print(\"1 batch\")\n",
    "    \n",
    "    loss_epoch.append(epochs)\n",
    "    loss_values.append(loss.item())\n",
    "    \n",
    "    if epochs%1==0:\n",
    "        print(\"Epochs: \", epochs, \"; Loss: \", loss.item())\n",
    "        \n",
    "    loss=loss.item()\n",
    "\n",
    "print(epochs, \"    \", loss.item())\n",
    "\n",
    "#Plot loss function\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(loss_epoch, loss_values)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc9dbba",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea4f7897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot loss function\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(loss_epoch, loss_values)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.semilogy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d43359",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f441f368",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [3, 1, 3, 3], but got 2-dimensional input of size [1, 6400] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      3\u001b[0m PATH\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_jit_uns_cnn_3.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m traced_net\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m6400\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m traced_net\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m      6\u001b[0m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39msave(traced_net, PATH)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_trace.py:741\u001b[0m, in \u001b[0;36mtrace\u001b[1;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func\n\u001b[0;32m    740\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m--> 741\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrap_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[0;32m    756\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    757\u001b[0m ):\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trace_module(\n\u001b[0;32m    759\u001b[0m         func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m,\n\u001b[0;32m    760\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m: example_inputs},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    767\u001b[0m         _module_class,\n\u001b[0;32m    768\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_trace.py:958\u001b[0m, in \u001b[0;36mtrace_module\u001b[1;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[0;32m    954\u001b[0m     argument_names \u001b[38;5;241m=\u001b[39m get_callable_argument_names(func)\n\u001b[0;32m    956\u001b[0m example_inputs \u001b[38;5;241m=\u001b[39m make_tuple(example_inputs)\n\u001b[1;32m--> 958\u001b[0m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_method_from_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvar_lookup_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43margument_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    967\u001b[0m check_trace_method \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_c\u001b[38;5;241m.\u001b[39m_get_method(method_name)\n\u001b[0;32m    969\u001b[0m \u001b[38;5;66;03m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1090\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1088\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1090\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mConvNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 18\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n\u001b[0;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# flatten all dimensions except the batch dimension\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1090\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1088\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1090\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:446\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:442\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    440\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    441\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [3, 1, 3, 3], but got 2-dimensional input of size [1, 6400] instead"
     ]
    }
   ],
   "source": [
    "device=torch.device('cpu')\n",
    "model=model.to(device)\n",
    "PATH= \"model_jit_uns_cnn_3.pth\"\n",
    "traced_net=torch.jit.trace(model, (torch.randn(1,6400)).to(device))\n",
    "traced_net.to(torch.float64)\n",
    "torch.jit.save(traced_net, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b811b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_net=torch.jit.trace(model, (torch.randn(1, 1, 80,80)).to(device))\n",
    "traced_net.to(torch.float64)\n",
    "torch.jit.save(traced_net, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14781153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
