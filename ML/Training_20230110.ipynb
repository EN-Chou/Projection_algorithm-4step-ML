{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6e43f9",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ebe9934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fad5224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda:1') #先調1再調0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0994207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe6f063",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ENCHOU\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\cuda\\__init__.py:120: UserWarning: \n",
      "    Found GPU%d %s which is of cuda capability %d.%d.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability supported by this library is %d.%d.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn.format(d, name, major, minor, min_arch // 10, min_arch % 10))\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(1000, 6724)\n",
    "x=x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34dbfa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce8c68f",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1157f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Prepare data-1/2\n",
    "# DataLoader wraps a Dataset and provides minibatches, shuffling, multithreading, for you\n",
    "data_in=np.loadtxt('./data/preserved/input_div_U_2_1s.dat')\n",
    "data_out=np.loadtxt('./data/preserved/input_div_U_2_1s.dat')\n",
    "x_in=torch.Tensor(data_in)\n",
    "y_in=torch.Tensor(data_out)\n",
    "x_in=x_in.to(device)\n",
    "y_in=y_in.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "849bbccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 6400])\n",
      "torch.Size([1000, 6400])\n"
     ]
    }
   ],
   "source": [
    "x=x_in\n",
    "y=y_in\n",
    "print(x.size())\n",
    "print(y.size())\n",
    "loader=DataLoader(TensorDataset(x, y), batch_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782c5516",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534a75bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Create model 建立model習慣建立class\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, A, B, C, D, E, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear_1=torch.nn.Linear(D_in, A)\n",
    "        self.linear_2=torch.nn.Linear(A, B)\n",
    "        self.linear_4=torch.nn.Linear(B, C)\n",
    "        self.linear_5=torch.nn.Linear(C, D)\n",
    "        self.linear_6=torch.nn.Linear(D, E)\n",
    "        self.linear_3=torch.nn.Linear(E, D_out)\n",
    "    \n",
    "    # Step 3. Forward pass-1/2    # Step 4. Backward pass-1/2\n",
    "    def forward(self, x):\n",
    "        a=self.linear_1(x)\n",
    "        a_relu=torch.nn.functional.relu(a) #為何activation and hidden layer 的實現方式不同\n",
    "        b=self.linear_2(a_relu) \n",
    "        b_relu=torch.nn.functional.relu(b)\n",
    "        c=self.linear_4(b_relu) \n",
    "        c_relu=torch.nn.functional.relu(c)\n",
    "        d=self.linear_5(c_relu) \n",
    "        d_relu=torch.nn.functional.relu(d)\n",
    "        e=self.linear_6(d_relu) \n",
    "        e_relu=torch.nn.functional.relu(e)\n",
    "        y_pred=self.linear_3(e_relu) \n",
    "        return y_pred\n",
    "    \n",
    "model= TwoLayerNet(D_in=6400, A=1000, B=1000, C=1000, D=1000, E=1000, D_out=6400)\n",
    "model=model.to(device) #這行是什麼意思? A:将模型加载到相应的设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed31c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VariationalEncoder(nn.Module):\n",
    "    def __init__(self, latent_dims):  \n",
    "        super(VariationalEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, 3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(4, 8, 3, stride=1, padding=1)\n",
    "        self.batch2 = nn.BatchNorm2d(8)\n",
    "        self.conv3 = nn.Conv2d(8, 16, 3, stride=1, padding=1)  \n",
    "        self.linear1 = nn.Linear(80*80*16, 6400)\n",
    "        self.linear2 = nn.Linear(6400, latent_dims)\n",
    "        self.linear3 = nn.Linear(6400, latent_dims)\n",
    "\n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "        self.N.loc = self.N.loc.cuda() # hack to get sampling on the GPU\n",
    "        self.N.scale = self.N.scale.cuda()\n",
    "        self.kl = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.batch2(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        mu =  self.linear2(x)\n",
    "        sigma = torch.exp(self.linear3(x))\n",
    "        z = mu + sigma*self.N.sample(mu.shape)\n",
    "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n",
    "        return z      \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            nn.Linear(latent_dims, 6400),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(6400, 80 * 80 * 16),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(16, 80, 80))\n",
    "\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, 8, 3, stride=1, output_padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 4, 3, stride=1, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(4, 1, 3, stride=1, padding=1, output_padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.decoder_lin(x)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.decoder_conv(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.encoder = VariationalEncoder(latent_dims)\n",
    "        self.decoder = Decoder(latent_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "    \n",
    "### Set the random seed for reproducible results\n",
    "torch.manual_seed(0)\n",
    "model = VariationalAutoencoder(latent_dims=128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a92f9540",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self, channel_1, channel_2, channel_3, channel_4, channel_5, kernel_dim):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, channel_1, kernel_dim)\n",
    "        self.conv2 = nn.Conv2d(channel_1, channel_2, kernel_dim)\n",
    "        self.conv3 = nn.Conv2d(channel_2, channel_3, kernel_dim)\n",
    "        self.conv4 = nn.Conv2d(channel_3, channel_4, kernel_dim)\n",
    "        self.conv5 = nn.Conv2d(channel_4, 1, kernel_dim)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(70*70*1, 6400)  # 78*78 from image dimension\n",
    "        #self.fc2 = nn.Linear(1000, 1000)\n",
    "        #self.fc3 = nn.Linear(1000, 6400)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "model = ConvNet(channel_1=3, channel_2=3, channel_3=3, channel_4=3, channel_5=3, kernel_dim=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58d1db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.nn.Sequential(torch.nn.Unflatten(1, (1,80,80)))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd496d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=DataLoader(TensorDataset(x, x), batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e42a038",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3229b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tol=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99f15f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.RMSprop(model.parameters(), lr=0.001, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "687717dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5000, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66d34a83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs    Loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ENCHOU\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "C:\\Users\\ENCHOU\\AppData\\Local\\Temp\\ipykernel_2696\\3864271103.py:33: UserWarning: Using a target size (torch.Size([100, 1, 80, 80])) that is different to the input size (torch.Size([100, 80, 80])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss=torch.nn.functional.mse_loss(L_y_pred, x_batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:  1 ; Loss:  0.27616599202156067\n",
      "Epochs:  2 ; Loss:  0.16618725657463074\n",
      "Epochs:  3 ; Loss:  0.09702491015195847\n",
      "Epochs:  4 ; Loss:  0.07098185271024704\n",
      "Epochs:  5 ; Loss:  0.05950101464986801\n",
      "Epochs:  6 ; Loss:  0.05330806225538254\n",
      "Epochs:  7 ; Loss:  0.0494338683784008\n",
      "Epochs:  8 ; Loss:  0.046534959226846695\n",
      "Epochs:  9 ; Loss:  0.04413104057312012\n",
      "Epochs:  10 ; Loss:  0.042040903121232986\n",
      "Epochs:  11 ; Loss:  0.04017295315861702\n",
      "Epochs:  12 ; Loss:  0.03847847133874893\n",
      "Epochs:  13 ; Loss:  0.03693147376179695\n",
      "Epochs:  14 ; Loss:  0.035511311143636703\n",
      "Epochs:  15 ; Loss:  0.03420316427946091\n",
      "Epochs:  16 ; Loss:  0.03299432620406151\n",
      "Epochs:  17 ; Loss:  0.03187435120344162\n",
      "Epochs:  18 ; Loss:  0.030834196135401726\n",
      "Epochs:  19 ; Loss:  0.029865877702832222\n",
      "Epochs:  20 ; Loss:  0.028962552547454834\n",
      "Epochs:  21 ; Loss:  0.028118200600147247\n",
      "Epochs:  22 ; Loss:  0.02732742950320244\n",
      "Epochs:  23 ; Loss:  0.026585519313812256\n",
      "Epochs:  24 ; Loss:  0.025888176634907722\n",
      "Epochs:  25 ; Loss:  0.025231605395674706\n",
      "Epochs:  26 ; Loss:  0.02461240254342556\n",
      "Epochs:  27 ; Loss:  0.024027463048696518\n",
      "Epochs:  28 ; Loss:  0.02347400411963463\n",
      "Epochs:  29 ; Loss:  0.022949516773223877\n",
      "Epochs:  30 ; Loss:  0.02245185151696205\n",
      "Epochs:  31 ; Loss:  0.02197861671447754\n",
      "Epochs:  32 ; Loss:  0.021528348326683044\n",
      "Epochs:  33 ; Loss:  0.02109898254275322\n",
      "Epochs:  34 ; Loss:  0.02068941853940487\n",
      "Epochs:  35 ; Loss:  0.020297599956393242\n",
      "Epochs:  36 ; Loss:  0.019922878593206406\n",
      "Epochs:  37 ; Loss:  0.019563868641853333\n",
      "Epochs:  38 ; Loss:  0.019219456240534782\n",
      "Epochs:  39 ; Loss:  0.018888743594288826\n",
      "Epochs:  40 ; Loss:  0.018570806831121445\n",
      "Epochs:  41 ; Loss:  0.018264828249812126\n",
      "Epochs:  42 ; Loss:  0.017970183864235878\n",
      "Epochs:  43 ; Loss:  0.017685871571302414\n",
      "Epochs:  44 ; Loss:  0.017411543056368828\n",
      "Epochs:  45 ; Loss:  0.017146563157439232\n",
      "Epochs:  46 ; Loss:  0.0168903861194849\n",
      "Epochs:  47 ; Loss:  0.016642531380057335\n",
      "Epochs:  48 ; Loss:  0.016402626410126686\n",
      "Epochs:  49 ; Loss:  0.016170121729373932\n",
      "Epochs:  50 ; Loss:  0.015944678336381912\n",
      "Epochs:  51 ; Loss:  0.01572582870721817\n",
      "Epochs:  52 ; Loss:  0.01551332138478756\n",
      "Epochs:  53 ; Loss:  0.015307029709219933\n",
      "Epochs:  54 ; Loss:  0.015106379054486752\n",
      "Epochs:  55 ; Loss:  0.014911344274878502\n",
      "Epochs:  56 ; Loss:  0.014721517451107502\n",
      "Epochs:  57 ; Loss:  0.014536775648593903\n",
      "Epochs:  58 ; Loss:  0.014356952160596848\n",
      "Epochs:  59 ; Loss:  0.014181512407958508\n",
      "Epochs:  60 ; Loss:  0.014010612852871418\n",
      "Epochs:  61 ; Loss:  0.013843998312950134\n",
      "Epochs:  62 ; Loss:  0.01368147786706686\n",
      "Epochs:  63 ; Loss:  0.013523027300834656\n",
      "Epochs:  64 ; Loss:  0.013368061743676662\n",
      "Epochs:  65 ; Loss:  0.013216794468462467\n",
      "Epochs:  66 ; Loss:  0.01306911464780569\n",
      "Epochs:  67 ; Loss:  0.012924876064062119\n",
      "Epochs:  68 ; Loss:  0.012784004211425781\n",
      "Epochs:  69 ; Loss:  0.012645954266190529\n",
      "Epochs:  70 ; Loss:  0.012511055916547775\n",
      "Epochs:  71 ; Loss:  0.01237910520285368\n",
      "Epochs:  72 ; Loss:  0.012250091880559921\n",
      "Epochs:  73 ; Loss:  0.01212392933666706\n",
      "Epochs:  74 ; Loss:  0.011999938637018204\n",
      "Epochs:  75 ; Loss:  0.01187892071902752\n",
      "Epochs:  76 ; Loss:  0.011760491877794266\n",
      "Epochs:  77 ; Loss:  0.011644028127193451\n",
      "Epochs:  78 ; Loss:  0.01153021864593029\n",
      "Epochs:  79 ; Loss:  0.011418373323976994\n",
      "Epochs:  80 ; Loss:  0.011308887973427773\n",
      "Epochs:  81 ; Loss:  0.011201512068510056\n",
      "Epochs:  82 ; Loss:  0.011096318252384663\n",
      "Epochs:  83 ; Loss:  0.010993179865181446\n",
      "Epochs:  84 ; Loss:  0.010892007499933243\n",
      "Epochs:  85 ; Loss:  0.010792609304189682\n",
      "Epochs:  86 ; Loss:  0.010695058852434158\n",
      "Epochs:  87 ; Loss:  0.010599453933537006\n",
      "Epochs:  88 ; Loss:  0.010505760088562965\n",
      "Epochs:  89 ; Loss:  0.01041332632303238\n",
      "Epochs:  90 ; Loss:  0.010322904214262962\n",
      "Epochs:  91 ; Loss:  0.01023414172232151\n",
      "Epochs:  92 ; Loss:  0.010147012770175934\n",
      "Epochs:  93 ; Loss:  0.010060971602797508\n",
      "Epochs:  94 ; Loss:  0.009976699016988277\n",
      "Epochs:  95 ; Loss:  0.0098938699811697\n",
      "Epochs:  96 ; Loss:  0.009812837466597557\n",
      "Epochs:  97 ; Loss:  0.009732361882925034\n",
      "Epochs:  98 ; Loss:  0.009653634391725063\n",
      "Epochs:  99 ; Loss:  0.00957642775028944\n",
      "Epochs:  100 ; Loss:  0.009500321000814438\n",
      "Epochs:  101 ; Loss:  0.009425484575331211\n",
      "Epochs:  102 ; Loss:  0.009351820684969425\n",
      "Epochs:  103 ; Loss:  0.009279492311179638\n",
      "Epochs:  104 ; Loss:  0.009208332747220993\n",
      "Epochs:  105 ; Loss:  0.009138375520706177\n",
      "Epochs:  106 ; Loss:  0.009069082327187061\n",
      "Epochs:  107 ; Loss:  0.009001826867461205\n",
      "Epochs:  108 ; Loss:  0.00893490668386221\n",
      "Epochs:  109 ; Loss:  0.008868729695677757\n",
      "Epochs:  110 ; Loss:  0.008805029094219208\n",
      "Epochs:  111 ; Loss:  0.008740843273699284\n",
      "Epochs:  112 ; Loss:  0.008677555248141289\n",
      "Epochs:  113 ; Loss:  0.008616259321570396\n",
      "Epochs:  114 ; Loss:  0.008554828353226185\n",
      "Epochs:  115 ; Loss:  0.008494368754327297\n",
      "Epochs:  116 ; Loss:  0.008435477502644062\n",
      "Epochs:  117 ; Loss:  0.008377214893698692\n",
      "Epochs:  118 ; Loss:  0.008319373242557049\n",
      "Epochs:  119 ; Loss:  0.008262761868536472\n",
      "Epochs:  120 ; Loss:  0.008207130245864391\n",
      "Epochs:  121 ; Loss:  0.008152147755026817\n",
      "Epochs:  122 ; Loss:  0.008098057471215725\n",
      "Epochs:  123 ; Loss:  0.008044063113629818\n",
      "Epochs:  124 ; Loss:  0.007991326041519642\n",
      "Epochs:  125 ; Loss:  0.007939361967146397\n",
      "Epochs:  126 ; Loss:  0.00788830779492855\n",
      "Epochs:  127 ; Loss:  0.00783757958561182\n",
      "Epochs:  128 ; Loss:  0.007787931244820356\n",
      "Epochs:  129 ; Loss:  0.007739142514765263\n",
      "Epochs:  130 ; Loss:  0.00769049022346735\n",
      "Epochs:  131 ; Loss:  0.007641809526830912\n",
      "Epochs:  132 ; Loss:  0.00759431067854166\n",
      "Epochs:  133 ; Loss:  0.007547600194811821\n",
      "Epochs:  134 ; Loss:  0.007501552347093821\n",
      "Epochs:  135 ; Loss:  0.007457267493009567\n",
      "Epochs:  136 ; Loss:  0.007412042003124952\n",
      "Epochs:  137 ; Loss:  0.007367087062448263\n",
      "Epochs:  138 ; Loss:  0.007323978468775749\n",
      "Epochs:  139 ; Loss:  0.007280218880623579\n",
      "Epochs:  140 ; Loss:  0.007237187586724758\n",
      "Epochs:  141 ; Loss:  0.007194638717919588\n",
      "Epochs:  142 ; Loss:  0.007153464015573263\n",
      "Epochs:  143 ; Loss:  0.007112305145710707\n",
      "Epochs:  144 ; Loss:  0.007072986103594303\n",
      "Epochs:  145 ; Loss:  0.007031597662717104\n",
      "Epochs:  146 ; Loss:  0.006991653703153133\n",
      "Epochs:  147 ; Loss:  0.006953006610274315\n",
      "Epochs:  148 ; Loss:  0.006914169527590275\n",
      "Epochs:  149 ; Loss:  0.006876199971884489\n",
      "Epochs:  150 ; Loss:  0.006838473025709391\n",
      "Epochs:  151 ; Loss:  0.006801017094403505\n",
      "Epochs:  152 ; Loss:  0.006764839868992567\n",
      "Epochs:  153 ; Loss:  0.00672837533056736\n",
      "Epochs:  154 ; Loss:  0.006691827438771725\n",
      "Epochs:  155 ; Loss:  0.0066557577811181545\n",
      "Epochs:  156 ; Loss:  0.0066201407462358475\n",
      "Epochs:  157 ; Loss:  0.0065854210406541824\n",
      "Epochs:  158 ; Loss:  0.006551118101924658\n",
      "Epochs:  159 ; Loss:  0.006517063360661268\n",
      "Epochs:  160 ; Loss:  0.006483388599008322\n",
      "Epochs:  161 ; Loss:  0.006449887063354254\n",
      "Epochs:  162 ; Loss:  0.0064169601537287235\n",
      "Epochs:  163 ; Loss:  0.006384661886841059\n",
      "Epochs:  164 ; Loss:  0.0063529182225465775\n",
      "Epochs:  165 ; Loss:  0.006320694461464882\n",
      "Epochs:  166 ; Loss:  0.006288289092481136\n",
      "Epochs:  167 ; Loss:  0.006257943343371153\n",
      "Epochs:  168 ; Loss:  0.0062264204025268555\n",
      "Epochs:  169 ; Loss:  0.006194834131747484\n",
      "Epochs:  170 ; Loss:  0.006164407357573509\n",
      "Epochs:  171 ; Loss:  0.006135712843388319\n",
      "Epochs:  172 ; Loss:  0.0061060581356287\n",
      "Epochs:  173 ; Loss:  0.006078047677874565\n",
      "Epochs:  174 ; Loss:  0.006048713810741901\n",
      "Epochs:  175 ; Loss:  0.006018838379532099\n",
      "Epochs:  176 ; Loss:  0.005989368073642254\n",
      "Epochs:  177 ; Loss:  0.0059624360874295235\n",
      "Epochs:  178 ; Loss:  0.005934355314821005\n",
      "Epochs:  179 ; Loss:  0.0059056710451841354\n",
      "Epochs:  180 ; Loss:  0.005879802629351616\n",
      "Epochs:  181 ; Loss:  0.0058508701622486115\n",
      "Epochs:  182 ; Loss:  0.005823027808219194\n",
      "Epochs:  183 ; Loss:  0.005797232035547495\n",
      "Epochs:  184 ; Loss:  0.005772502161562443\n",
      "Epochs:  185 ; Loss:  0.005746213719248772\n",
      "Epochs:  186 ; Loss:  0.005718357861042023\n",
      "Epochs:  187 ; Loss:  0.005693324841558933\n",
      "Epochs:  188 ; Loss:  0.005668209865689278\n",
      "Epochs:  189 ; Loss:  0.005642905365675688\n",
      "Epochs:  190 ; Loss:  0.005617951042950153\n",
      "Epochs:  191 ; Loss:  0.00559595599770546\n",
      "Epochs:  192 ; Loss:  0.00556890619918704\n",
      "Epochs:  193 ; Loss:  0.005544576328247786\n",
      "Epochs:  194 ; Loss:  0.00551978824660182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:  195 ; Loss:  0.005498790182173252\n",
      "Epochs:  196 ; Loss:  0.005474988836795092\n",
      "Epochs:  197 ; Loss:  0.0054504116997122765\n",
      "Epochs:  198 ; Loss:  0.005427065305411816\n",
      "Epochs:  199 ; Loss:  0.005406143609434366\n",
      "Epochs:  200 ; Loss:  0.005379947368055582\n",
      "Epochs:  201 ; Loss:  0.005359186790883541\n",
      "Epochs:  202 ; Loss:  0.005334867630153894\n",
      "Epochs:  203 ; Loss:  0.005312602035701275\n",
      "Epochs:  204 ; Loss:  0.005290021654218435\n",
      "Epochs:  205 ; Loss:  0.005268737208098173\n",
      "Epochs:  206 ; Loss:  0.005246866960078478\n",
      "Epochs:  207 ; Loss:  0.005224473308771849\n",
      "Epochs:  208 ; Loss:  0.005204889457672834\n",
      "Epochs:  209 ; Loss:  0.005181438289582729\n",
      "Epochs:  210 ; Loss:  0.005163371097296476\n",
      "Epochs:  211 ; Loss:  0.005142617505043745\n",
      "Epochs:  212 ; Loss:  0.005120628047734499\n",
      "Epochs:  213 ; Loss:  0.005099035333842039\n",
      "Epochs:  214 ; Loss:  0.005082018673419952\n",
      "Epochs:  215 ; Loss:  0.005062530282884836\n",
      "Epochs:  216 ; Loss:  0.005038061179220676\n",
      "Epochs:  217 ; Loss:  0.005022027529776096\n",
      "Epochs:  218 ; Loss:  0.00499796262010932\n",
      "Epochs:  219 ; Loss:  0.004978624172508717\n",
      "Epochs:  220 ; Loss:  0.004962403327226639\n",
      "Epochs:  221 ; Loss:  0.004941264633089304\n",
      "Epochs:  222 ; Loss:  0.004920824430882931\n",
      "Epochs:  223 ; Loss:  0.004902716726064682\n",
      "Epochs:  224 ; Loss:  0.004884298890829086\n",
      "Epochs:  225 ; Loss:  0.004866396076977253\n",
      "Epochs:  226 ; Loss:  0.004846738185733557\n",
      "Epochs:  227 ; Loss:  0.004827267024666071\n",
      "Epochs:  228 ; Loss:  0.004807197954505682\n",
      "Epochs:  229 ; Loss:  0.004790001083165407\n",
      "Epochs:  230 ; Loss:  0.004773859400302172\n",
      "Epochs:  231 ; Loss:  0.004757270682603121\n",
      "Epochs:  232 ; Loss:  0.004744736012071371\n",
      "Epochs:  233 ; Loss:  0.004723867401480675\n",
      "Epochs:  234 ; Loss:  0.004702349193394184\n",
      "Epochs:  235 ; Loss:  0.004686322528868914\n",
      "Epochs:  236 ; Loss:  0.0046675242483615875\n",
      "Epochs:  237 ; Loss:  0.004648097325116396\n",
      "Epochs:  238 ; Loss:  0.004637783393263817\n",
      "Epochs:  239 ; Loss:  0.004613901022821665\n",
      "Epochs:  240 ; Loss:  0.004598814994096756\n",
      "Epochs:  241 ; Loss:  0.004585865419358015\n",
      "Epochs:  242 ; Loss:  0.004567463882267475\n",
      "Epochs:  243 ; Loss:  0.004550285171717405\n",
      "Epochs:  244 ; Loss:  0.004534165840595961\n",
      "Epochs:  245 ; Loss:  0.004516832064837217\n",
      "Epochs:  246 ; Loss:  0.0044983504340052605\n",
      "Epochs:  247 ; Loss:  0.004484519362449646\n",
      "Epochs:  248 ; Loss:  0.004467554856091738\n",
      "Epochs:  249 ; Loss:  0.0044551268219947815\n",
      "Epochs:  250 ; Loss:  0.00444653257727623\n",
      "Epochs:  251 ; Loss:  0.004427779465913773\n",
      "Epochs:  252 ; Loss:  0.004404745064675808\n",
      "Epochs:  253 ; Loss:  0.004388481378555298\n",
      "Epochs:  254 ; Loss:  0.0043752421624958515\n",
      "Epochs:  255 ; Loss:  0.004359426908195019\n",
      "Epochs:  256 ; Loss:  0.0043487842194736\n",
      "Epochs:  257 ; Loss:  0.0043367743492126465\n",
      "Epochs:  258 ; Loss:  0.004314780235290527\n",
      "Epochs:  259 ; Loss:  0.004309404641389847\n",
      "Epochs:  260 ; Loss:  0.0042921691201627254\n",
      "Epochs:  261 ; Loss:  0.004268660210072994\n",
      "Epochs:  262 ; Loss:  0.004256651736795902\n",
      "Epochs:  263 ; Loss:  0.0042446330189704895\n",
      "Epochs:  264 ; Loss:  0.004227315075695515\n",
      "Epochs:  265 ; Loss:  0.004214972257614136\n",
      "Epochs:  266 ; Loss:  0.004211099818348885\n",
      "Epochs:  267 ; Loss:  0.004187912214547396\n",
      "Epochs:  268 ; Loss:  0.0041714985854923725\n",
      "Epochs:  269 ; Loss:  0.0041625844314694405\n",
      "Epochs:  270 ; Loss:  0.0041448683477938175\n",
      "Epochs:  271 ; Loss:  0.004136121366173029\n",
      "Epochs:  272 ; Loss:  0.0041196150705218315\n",
      "Epochs:  273 ; Loss:  0.0041021425276994705\n",
      "Epochs:  274 ; Loss:  0.004088865593075752\n",
      "Epochs:  275 ; Loss:  0.004076020326465368\n",
      "Epochs:  276 ; Loss:  0.0040674894116818905\n",
      "Epochs:  277 ; Loss:  0.004056798294186592\n",
      "Epochs:  278 ; Loss:  0.004043270368129015\n",
      "Epochs:  279 ; Loss:  0.004048587288707495\n",
      "Epochs:  280 ; Loss:  0.004021229688078165\n",
      "Epochs:  281 ; Loss:  0.004000402055680752\n",
      "Epochs:  282 ; Loss:  0.003987947478890419\n",
      "Epochs:  283 ; Loss:  0.003978815395385027\n",
      "Epochs:  284 ; Loss:  0.003961651585996151\n",
      "Epochs:  285 ; Loss:  0.0039531392976641655\n",
      "Epochs:  286 ; Loss:  0.003939650021493435\n",
      "Epochs:  287 ; Loss:  0.003924110438674688\n",
      "Epochs:  288 ; Loss:  0.00390804884955287\n",
      "Epochs:  289 ; Loss:  0.003896149341017008\n",
      "Epochs:  290 ; Loss:  0.0038912231102585793\n",
      "Epochs:  291 ; Loss:  0.003903512842953205\n",
      "Epochs:  292 ; Loss:  0.0038795059081166983\n",
      "Epochs:  293 ; Loss:  0.0038581686094403267\n",
      "Epochs:  294 ; Loss:  0.0038394455332309008\n",
      "Epochs:  295 ; Loss:  0.003828242653980851\n",
      "Epochs:  296 ; Loss:  0.003819638630375266\n",
      "Epochs:  297 ; Loss:  0.0038078974466770887\n",
      "Epochs:  298 ; Loss:  0.0037986517418175936\n",
      "Epochs:  299 ; Loss:  0.0037919282913208008\n",
      "Epochs:  300 ; Loss:  0.003779955441132188\n",
      "Epochs:  301 ; Loss:  0.003755927085876465\n",
      "Epochs:  302 ; Loss:  0.0037537519820034504\n",
      "Epochs:  303 ; Loss:  0.003740079002454877\n",
      "Epochs:  304 ; Loss:  0.003729973454028368\n",
      "Epochs:  305 ; Loss:  0.0037388044875115156\n",
      "Epochs:  306 ; Loss:  0.0037113502621650696\n",
      "Epochs:  307 ; Loss:  0.003703763708472252\n",
      "Epochs:  308 ; Loss:  0.003686795476824045\n",
      "Epochs:  309 ; Loss:  0.003681065747514367\n",
      "Epochs:  310 ; Loss:  0.003664091695100069\n",
      "Epochs:  311 ; Loss:  0.003651577979326248\n",
      "Epochs:  312 ; Loss:  0.003642052412033081\n",
      "Epochs:  313 ; Loss:  0.0036407120060175657\n",
      "Epochs:  314 ; Loss:  0.003625298850238323\n",
      "Epochs:  315 ; Loss:  0.003609569277614355\n",
      "Epochs:  316 ; Loss:  0.003603374119848013\n",
      "Epochs:  317 ; Loss:  0.0035992346238344908\n",
      "Epochs:  318 ; Loss:  0.0035843197256326675\n",
      "Epochs:  319 ; Loss:  0.0035854224115610123\n",
      "Epochs:  320 ; Loss:  0.0035653237719088793\n",
      "Epochs:  321 ; Loss:  0.0035444730892777443\n",
      "Epochs:  322 ; Loss:  0.003539146389812231\n",
      "Epochs:  323 ; Loss:  0.0035445410758256912\n",
      "Epochs:  324 ; Loss:  0.0035338294692337513\n",
      "Epochs:  325 ; Loss:  0.0035070686135441065\n",
      "Epochs:  326 ; Loss:  0.003497102065011859\n",
      "Epochs:  327 ; Loss:  0.0034914903808385134\n",
      "Epochs:  328 ; Loss:  0.003492007264867425\n",
      "Epochs:  329 ; Loss:  0.0034802549052983522\n",
      "Epochs:  330 ; Loss:  0.0034695174545049667\n",
      "Epochs:  331 ; Loss:  0.003454081015661359\n",
      "Epochs:  332 ; Loss:  0.003438842250034213\n",
      "Epochs:  333 ; Loss:  0.0034296747762709856\n",
      "Epochs:  334 ; Loss:  0.0034475272987037897\n",
      "Epochs:  335 ; Loss:  0.003429135773330927\n",
      "Epochs:  336 ; Loss:  0.0034189331345260143\n",
      "Epochs:  337 ; Loss:  0.0033979997970163822\n",
      "Epochs:  338 ; Loss:  0.0033897720277309418\n",
      "Epochs:  339 ; Loss:  0.0033716850448399782\n",
      "Epochs:  340 ; Loss:  0.003367411671206355\n",
      "Epochs:  341 ; Loss:  0.0033770629670470953\n",
      "Epochs:  342 ; Loss:  0.003362704999744892\n",
      "Epochs:  343 ; Loss:  0.003348166588693857\n",
      "Epochs:  344 ; Loss:  0.0033381872344762087\n",
      "Epochs:  345 ; Loss:  0.0033237857278436422\n",
      "Epochs:  346 ; Loss:  0.0033118335995823145\n",
      "Epochs:  347 ; Loss:  0.0033059322740882635\n",
      "Epochs:  348 ; Loss:  0.0033092270605266094\n",
      "Epochs:  349 ; Loss:  0.003291376167908311\n",
      "Epochs:  350 ; Loss:  0.0032917391508817673\n",
      "Epochs:  351 ; Loss:  0.0032933021429926157\n",
      "Epochs:  352 ; Loss:  0.0032809542026370764\n",
      "Epochs:  353 ; Loss:  0.0032600664999336004\n",
      "Epochs:  354 ; Loss:  0.003238832112401724\n",
      "Epochs:  355 ; Loss:  0.003242677543312311\n",
      "Epochs:  356 ; Loss:  0.0032386835664510727\n",
      "Epochs:  357 ; Loss:  0.0032461201772093773\n",
      "Epochs:  358 ; Loss:  0.0032389892730861902\n",
      "Epochs:  359 ; Loss:  0.0032067582942545414\n",
      "Epochs:  360 ; Loss:  0.003194606862962246\n",
      "Epochs:  361 ; Loss:  0.00319051556289196\n",
      "Epochs:  362 ; Loss:  0.0031859667506068945\n",
      "Epochs:  363 ; Loss:  0.0031841464806348085\n",
      "Epochs:  364 ; Loss:  0.003193850629031658\n",
      "Epochs:  365 ; Loss:  0.003164899069815874\n",
      "Epochs:  366 ; Loss:  0.003145672148093581\n",
      "Epochs:  367 ; Loss:  0.003140360815450549\n",
      "Epochs:  368 ; Loss:  0.0031482642516493797\n",
      "Epochs:  369 ; Loss:  0.0031416260171681643\n",
      "Epochs:  370 ; Loss:  0.003144444664940238\n",
      "Epochs:  371 ; Loss:  0.003136229934170842\n",
      "Epochs:  372 ; Loss:  0.0031163853127509356\n",
      "Epochs:  373 ; Loss:  0.0030870672781020403\n",
      "Epochs:  374 ; Loss:  0.0030819005332887173\n",
      "Epochs:  375 ; Loss:  0.0030814697965979576\n",
      "Epochs:  376 ; Loss:  0.003097105072811246\n",
      "Epochs:  377 ; Loss:  0.00308709847740829\n",
      "Epochs:  378 ; Loss:  0.003072963561862707\n",
      "Epochs:  379 ; Loss:  0.0030397011432796717\n",
      "Epochs:  380 ; Loss:  0.003038077149540186\n",
      "Epochs:  381 ; Loss:  0.0030444725416600704\n",
      "Epochs:  382 ; Loss:  0.003056162968277931\n",
      "Epochs:  383 ; Loss:  0.0030387225560843945\n",
      "Epochs:  384 ; Loss:  0.0030167438089847565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:  385 ; Loss:  0.003003861289471388\n",
      "Epochs:  386 ; Loss:  0.003006239887326956\n",
      "Epochs:  387 ; Loss:  0.0030147333163768053\n",
      "Epochs:  388 ; Loss:  0.003025450510904193\n",
      "Epochs:  389 ; Loss:  0.003005783772096038\n",
      "Epochs:  390 ; Loss:  0.002962857950478792\n",
      "Epochs:  391 ; Loss:  0.002951074857264757\n",
      "Epochs:  392 ; Loss:  0.002947057830169797\n",
      "Epochs:  393 ; Loss:  0.0029751418624073267\n",
      "Epochs:  394 ; Loss:  0.0029879393987357616\n",
      "Epochs:  395 ; Loss:  0.0029603189323097467\n",
      "Epochs:  396 ; Loss:  0.002977619878947735\n",
      "Epochs:  397 ; Loss:  0.002941292244940996\n",
      "Epochs:  398 ; Loss:  0.0029089446179568768\n",
      "Epochs:  399 ; Loss:  0.002893800614401698\n",
      "Epochs:  400 ; Loss:  0.0028950078412890434\n",
      "Epochs:  401 ; Loss:  0.002918716287240386\n",
      "Epochs:  402 ; Loss:  0.002936634700745344\n",
      "Epochs:  403 ; Loss:  0.002900030231103301\n",
      "Epochs:  404 ; Loss:  0.0028833579272031784\n",
      "Epochs:  405 ; Loss:  0.002880952786654234\n",
      "Epochs:  406 ; Loss:  0.0028620893135666847\n",
      "Epochs:  407 ; Loss:  0.002870385767892003\n",
      "Epochs:  408 ; Loss:  0.002890479052439332\n",
      "Epochs:  409 ; Loss:  0.00288895214907825\n",
      "Epochs:  410 ; Loss:  0.0028676860965788364\n",
      "Epochs:  411 ; Loss:  0.0028412470128387213\n",
      "Epochs:  412 ; Loss:  0.0028274799697101116\n",
      "Epochs:  413 ; Loss:  0.00281912786886096\n",
      "Epochs:  414 ; Loss:  0.0028091534040868282\n",
      "Epochs:  415 ; Loss:  0.002835638588294387\n",
      "Epochs:  416 ; Loss:  0.0028282494749873877\n",
      "Epochs:  417 ; Loss:  0.0028794363606721163\n",
      "Epochs:  418 ; Loss:  0.002811291953548789\n",
      "Epochs:  419 ; Loss:  0.0027946035843342543\n",
      "Epochs:  420 ; Loss:  0.0027813243214040995\n",
      "Epochs:  421 ; Loss:  0.002759609604254365\n",
      "Epochs:  422 ; Loss:  0.0027605844661593437\n",
      "Epochs:  423 ; Loss:  0.0027679072227329016\n",
      "Epochs:  424 ; Loss:  0.0027690334245562553\n",
      "Epochs:  425 ; Loss:  0.0028224338311702013\n",
      "Epochs:  426 ; Loss:  0.0027715752366930246\n",
      "Epochs:  427 ; Loss:  0.0027475461829453707\n",
      "Epochs:  428 ; Loss:  0.0027332098688930273\n",
      "Epochs:  429 ; Loss:  0.0027385284192860126\n",
      "Epochs:  430 ; Loss:  0.002730247098952532\n",
      "Epochs:  431 ; Loss:  0.0027481941506266594\n",
      "Epochs:  432 ; Loss:  0.0027387042064219713\n",
      "Epochs:  433 ; Loss:  0.0027074271347373724\n",
      "Epochs:  434 ; Loss:  0.002689056796953082\n",
      "Epochs:  435 ; Loss:  0.0027025595773011446\n",
      "Epochs:  436 ; Loss:  0.002725082915276289\n",
      "Epochs:  437 ; Loss:  0.0027068345807492733\n",
      "Epochs:  438 ; Loss:  0.002701936988160014\n",
      "Epochs:  439 ; Loss:  0.002661067759618163\n",
      "Epochs:  440 ; Loss:  0.0026594267692416906\n",
      "Epochs:  441 ; Loss:  0.0027048937045037746\n",
      "Epochs:  442 ; Loss:  0.0026804981753230095\n",
      "Epochs:  443 ; Loss:  0.0026756722945719957\n",
      "Epochs:  444 ; Loss:  0.002666155807673931\n",
      "Epochs:  445 ; Loss:  0.002636071527376771\n",
      "Epochs:  446 ; Loss:  0.0026260740123689175\n",
      "Epochs:  447 ; Loss:  0.002650805748999119\n",
      "Epochs:  448 ; Loss:  0.002669085282832384\n",
      "Epochs:  449 ; Loss:  0.002637588419020176\n",
      "Epochs:  450 ; Loss:  0.0026391588617116213\n",
      "Epochs:  451 ; Loss:  0.002618810161948204\n",
      "Epochs:  452 ; Loss:  0.0026151654310524464\n",
      "Epochs:  453 ; Loss:  0.002591380849480629\n",
      "Epochs:  454 ; Loss:  0.002581821521744132\n",
      "Epochs:  455 ; Loss:  0.0026488325092941523\n",
      "Epochs:  456 ; Loss:  0.002660703146830201\n",
      "Epochs:  457 ; Loss:  0.002608590293675661\n",
      "Epochs:  458 ; Loss:  0.0025875309947878122\n",
      "Epochs:  459 ; Loss:  0.002553917234763503\n",
      "Epochs:  460 ; Loss:  0.002551817335188389\n",
      "Epochs:  461 ; Loss:  0.00256835063919425\n",
      "Epochs:  462 ; Loss:  0.002592492150142789\n",
      "Epochs:  463 ; Loss:  0.0025572036392986774\n",
      "Epochs:  464 ; Loss:  0.002541251014918089\n",
      "Epochs:  465 ; Loss:  0.0025336192920804024\n",
      "Epochs:  466 ; Loss:  0.002565797884017229\n",
      "Epochs:  467 ; Loss:  0.002542284084483981\n",
      "Epochs:  468 ; Loss:  0.0025519102346152067\n",
      "Epochs:  469 ; Loss:  0.0025209705345332623\n",
      "Epochs:  470 ; Loss:  0.002532775979489088\n",
      "Epochs:  471 ; Loss:  0.002562721259891987\n",
      "Epochs:  472 ; Loss:  0.0025608076248317957\n",
      "Epochs:  473 ; Loss:  0.0025199498049914837\n",
      "Epochs:  474 ; Loss:  0.002480457304045558\n",
      "Epochs:  475 ; Loss:  0.0024854165967553854\n",
      "Epochs:  476 ; Loss:  0.0024925421457737684\n",
      "Epochs:  477 ; Loss:  0.002527406206354499\n",
      "Epochs:  478 ; Loss:  0.0025532834697514772\n",
      "Epochs:  479 ; Loss:  0.002508132252842188\n",
      "Epochs:  480 ; Loss:  0.0024637088645249605\n",
      "Epochs:  481 ; Loss:  0.0024497744161635637\n",
      "Epochs:  482 ; Loss:  0.0025063129141926765\n",
      "Epochs:  483 ; Loss:  0.0024888881016522646\n",
      "Epochs:  484 ; Loss:  0.002473538974300027\n",
      "Epochs:  485 ; Loss:  0.002453017979860306\n",
      "Epochs:  486 ; Loss:  0.002437034621834755\n",
      "Epochs:  487 ; Loss:  0.0024210878182202578\n",
      "Epochs:  488 ; Loss:  0.002436291892081499\n",
      "Epochs:  489 ; Loss:  0.0024930306244641542\n",
      "Epochs:  490 ; Loss:  0.0024736514315009117\n",
      "Epochs:  491 ; Loss:  0.0024220484774559736\n",
      "Epochs:  492 ; Loss:  0.002401443663984537\n",
      "Epochs:  493 ; Loss:  0.0024071151856333017\n",
      "Epochs:  494 ; Loss:  0.002461446216329932\n",
      "Epochs:  495 ; Loss:  0.002427619183436036\n",
      "Epochs:  496 ; Loss:  0.0024201669730246067\n",
      "Epochs:  497 ; Loss:  0.002397520001977682\n",
      "Epochs:  498 ; Loss:  0.0024129878729581833\n",
      "Epochs:  499 ; Loss:  0.0024145476054400206\n",
      "Epochs:  500 ; Loss:  0.0023967616725713015\n",
      "Epochs:  501 ; Loss:  0.0023830418940633535\n",
      "Epochs:  502 ; Loss:  0.002359219826757908\n",
      "Epochs:  503 ; Loss:  0.0023795990273356438\n",
      "Epochs:  504 ; Loss:  0.002406358951702714\n",
      "Epochs:  505 ; Loss:  0.0024153466802090406\n",
      "Epochs:  506 ; Loss:  0.002437606919556856\n",
      "Epochs:  507 ; Loss:  0.0023512933403253555\n",
      "Epochs:  508 ; Loss:  0.0023297469597309828\n",
      "Epochs:  509 ; Loss:  0.0023239883594214916\n",
      "Epochs:  510 ; Loss:  0.002385307103395462\n",
      "Epochs:  511 ; Loss:  0.0024226661771535873\n",
      "Epochs:  512 ; Loss:  0.0023792895954102278\n",
      "Epochs:  513 ; Loss:  0.0023353148717433214\n",
      "Epochs:  514 ; Loss:  0.002324672183021903\n",
      "Epochs:  515 ; Loss:  0.0023064527194947004\n",
      "Epochs:  516 ; Loss:  0.002300637774169445\n",
      "Epochs:  517 ; Loss:  0.0023158073890954256\n",
      "Epochs:  518 ; Loss:  0.0023544151335954666\n",
      "Epochs:  519 ; Loss:  0.0024208081886172295\n",
      "Epochs:  520 ; Loss:  0.0023319178726524115\n",
      "Epochs:  521 ; Loss:  0.00229042861610651\n",
      "Epochs:  522 ; Loss:  0.0022684880532324314\n",
      "Epochs:  523 ; Loss:  0.002264088485389948\n",
      "Epochs:  524 ; Loss:  0.0023407512344419956\n",
      "Epochs:  525 ; Loss:  0.0023738988675177097\n",
      "Epochs:  526 ; Loss:  0.0023379286285489798\n",
      "Epochs:  527 ; Loss:  0.002295370679348707\n",
      "Epochs:  528 ; Loss:  0.0022539549972862005\n",
      "Epochs:  529 ; Loss:  0.0022406280040740967\n",
      "Epochs:  530 ; Loss:  0.0022607347927987576\n",
      "Epochs:  531 ; Loss:  0.0022712473291903734\n",
      "Epochs:  532 ; Loss:  0.002249988727271557\n",
      "Epochs:  533 ; Loss:  0.002248757751658559\n",
      "Epochs:  534 ; Loss:  0.0023445142433047295\n",
      "Epochs:  535 ; Loss:  0.002277562627568841\n",
      "Epochs:  536 ; Loss:  0.0022471118718385696\n",
      "Epochs:  537 ; Loss:  0.0022431283723562956\n",
      "Epochs:  538 ; Loss:  0.0023066336289048195\n",
      "Epochs:  539 ; Loss:  0.0022348230704665184\n",
      "Epochs:  540 ; Loss:  0.002233010483905673\n",
      "Epochs:  541 ; Loss:  0.0022318693809211254\n",
      "Epochs:  542 ; Loss:  0.0022294740192592144\n",
      "Epochs:  543 ; Loss:  0.00227801944129169\n",
      "Epochs:  544 ; Loss:  0.0022883075289428234\n",
      "Epochs:  545 ; Loss:  0.0022199004888534546\n",
      "Epochs:  546 ; Loss:  0.0021931191440671682\n",
      "Epochs:  547 ; Loss:  0.0021970183588564396\n",
      "Epochs:  548 ; Loss:  0.0021963114850223064\n",
      "Epochs:  549 ; Loss:  0.0021979715675115585\n",
      "Epochs:  550 ; Loss:  0.002295608399435878\n",
      "Epochs:  551 ; Loss:  0.002246482064947486\n",
      "Epochs:  552 ; Loss:  0.002238392597064376\n",
      "Epochs:  553 ; Loss:  0.0021883524022996426\n",
      "Epochs:  554 ; Loss:  0.0021835181396454573\n",
      "Epochs:  555 ; Loss:  0.002167759696021676\n",
      "Epochs:  556 ; Loss:  0.0022147998679429293\n",
      "Epochs:  557 ; Loss:  0.002220274182036519\n",
      "Epochs:  558 ; Loss:  0.0021990034729242325\n",
      "Epochs:  559 ; Loss:  0.0021462657023221254\n",
      "Epochs:  560 ; Loss:  0.0021293163299560547\n",
      "Epochs:  561 ; Loss:  0.0021307531278580427\n",
      "Epochs:  562 ; Loss:  0.0022180569358170033\n",
      "Epochs:  563 ; Loss:  0.0022971611469984055\n",
      "Epochs:  564 ; Loss:  0.002235759748145938\n",
      "Epochs:  565 ; Loss:  0.0021576809231191874\n",
      "Epochs:  566 ; Loss:  0.0021183760836720467\n",
      "Epochs:  567 ; Loss:  0.002105958526954055\n",
      "Epochs:  568 ; Loss:  0.002098732627928257\n",
      "Epochs:  569 ; Loss:  0.0020969146862626076\n",
      "Epochs:  570 ; Loss:  0.0021193388383835554\n",
      "Epochs:  571 ; Loss:  0.0022355744149535894\n",
      "Epochs:  572 ; Loss:  0.002276601502671838\n",
      "Epochs:  573 ; Loss:  0.002162208314985037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:  574 ; Loss:  0.002107859356328845\n",
      "Epochs:  575 ; Loss:  0.0020892249885946512\n",
      "Epochs:  576 ; Loss:  0.0021119946613907814\n",
      "Epochs:  577 ; Loss:  0.002133661648258567\n",
      "Epochs:  578 ; Loss:  0.0021100298035889864\n",
      "Epochs:  579 ; Loss:  0.0020759901963174343\n",
      "Epochs:  580 ; Loss:  0.0020670597441494465\n",
      "Epochs:  581 ; Loss:  0.0020871178712695837\n",
      "Epochs:  582 ; Loss:  0.002151709981262684\n",
      "Epochs:  583 ; Loss:  0.002161681652069092\n",
      "Epochs:  584 ; Loss:  0.0021371603943407536\n",
      "Epochs:  585 ; Loss:  0.0020961430855095387\n",
      "Epochs:  586 ; Loss:  0.0020661447197198868\n",
      "Epochs:  587 ; Loss:  0.00206693890504539\n",
      "Epochs:  588 ; Loss:  0.002133185276761651\n",
      "Epochs:  589 ; Loss:  0.002191901905462146\n",
      "Epochs:  590 ; Loss:  0.0021100181620568037\n",
      "Epochs:  591 ; Loss:  0.0020607328042387962\n",
      "Epochs:  592 ; Loss:  0.0020742809865623713\n",
      "Epochs:  593 ; Loss:  0.002046857727691531\n",
      "Epochs:  594 ; Loss:  0.002065698616206646\n",
      "Epochs:  595 ; Loss:  0.0020982606802135706\n",
      "Epochs:  596 ; Loss:  0.0020597358234226704\n",
      "Epochs:  597 ; Loss:  0.0020956690423190594\n",
      "Epochs:  598 ; Loss:  0.0021338420920073986\n",
      "Epochs:  599 ; Loss:  0.002037656959146261\n",
      "Epochs:  600 ; Loss:  0.002022701548412442\n",
      "Epochs:  601 ; Loss:  0.002039144281297922\n",
      "Epochs:  602 ; Loss:  0.002077119192108512\n",
      "Epochs:  603 ; Loss:  0.00207566749304533\n",
      "Epochs:  604 ; Loss:  0.002049975097179413\n",
      "Epochs:  605 ; Loss:  0.002034911420196295\n",
      "Epochs:  606 ; Loss:  0.0020199299324303865\n",
      "Epochs:  607 ; Loss:  0.0020418339408934116\n",
      "Epochs:  608 ; Loss:  0.002017668914049864\n",
      "Epochs:  609 ; Loss:  0.0020600694697350264\n",
      "Epochs:  610 ; Loss:  0.0020637905690819025\n",
      "Epochs:  611 ; Loss:  0.0020463475957512856\n",
      "Epochs:  612 ; Loss:  0.0020402688533067703\n",
      "Epochs:  613 ; Loss:  0.002021858701482415\n",
      "Epochs:  614 ; Loss:  0.0019835676066577435\n",
      "Epochs:  615 ; Loss:  0.0019785503391176462\n",
      "Epochs:  616 ; Loss:  0.0020495951175689697\n",
      "Epochs:  617 ; Loss:  0.0021156121511012316\n",
      "Epochs:  618 ; Loss:  0.002072830218821764\n",
      "Epochs:  619 ; Loss:  0.001994100399315357\n",
      "Epochs:  620 ; Loss:  0.0019714583177119493\n",
      "Epochs:  621 ; Loss:  0.001969150034710765\n",
      "Epochs:  622 ; Loss:  0.0019673937931656837\n",
      "Epochs:  623 ; Loss:  0.0019549927674233913\n",
      "Epochs:  624 ; Loss:  0.002005820395424962\n",
      "Epochs:  625 ; Loss:  0.0020424439571797848\n",
      "Epochs:  626 ; Loss:  0.0020047356374561787\n",
      "Epochs:  627 ; Loss:  0.00202774815261364\n",
      "Epochs:  628 ; Loss:  0.002029122319072485\n",
      "Epochs:  629 ; Loss:  0.00196687295101583\n",
      "Epochs:  630 ; Loss:  0.0019396061543375254\n",
      "Epochs:  631 ; Loss:  0.0019477554596960545\n",
      "Epochs:  632 ; Loss:  0.002039158483967185\n",
      "Epochs:  633 ; Loss:  0.0020391377620399\n",
      "Epochs:  634 ; Loss:  0.002041542436927557\n",
      "Epochs:  635 ; Loss:  0.001970744226127863\n",
      "Epochs:  636 ; Loss:  0.0019308552145957947\n",
      "Epochs:  637 ; Loss:  0.0019140834920108318\n",
      "Epochs:  638 ; Loss:  0.0019168518483638763\n",
      "Epochs:  639 ; Loss:  0.0019883927889168262\n",
      "Epochs:  640 ; Loss:  0.001993391662836075\n",
      "Epochs:  641 ; Loss:  0.001954335719347\n",
      "Epochs:  642 ; Loss:  0.001926662866026163\n",
      "Epochs:  643 ; Loss:  0.0019221875118091702\n",
      "Epochs:  644 ; Loss:  0.001927358447574079\n",
      "Epochs:  645 ; Loss:  0.0019468346145004034\n",
      "Epochs:  646 ; Loss:  0.001961048226803541\n",
      "Epochs:  647 ; Loss:  0.0019138173665851355\n",
      "Epochs:  648 ; Loss:  0.0019403108162805438\n",
      "Epochs:  649 ; Loss:  0.0019921883940696716\n",
      "Epochs:  650 ; Loss:  0.0019299499690532684\n",
      "Epochs:  651 ; Loss:  0.0019355837721377611\n",
      "Epochs:  652 ; Loss:  0.0019043672364205122\n",
      "Epochs:  653 ; Loss:  0.0019285556627437472\n",
      "Epochs:  654 ; Loss:  0.0019731924403458834\n",
      "Epochs:  655 ; Loss:  0.001950861420482397\n",
      "Epochs:  656 ; Loss:  0.0019189395243301988\n",
      "Epochs:  657 ; Loss:  0.0019136948976665735\n",
      "Epochs:  658 ; Loss:  0.001893329550512135\n",
      "Epochs:  659 ; Loss:  0.00189253780990839\n",
      "Epochs:  660 ; Loss:  0.0018955384148284793\n",
      "Epochs:  661 ; Loss:  0.001921282964758575\n",
      "Epochs:  662 ; Loss:  0.0019637763034552336\n",
      "Epochs:  663 ; Loss:  0.0019078103359788656\n",
      "Epochs:  664 ; Loss:  0.0018903851741924882\n",
      "Epochs:  665 ; Loss:  0.0018840414704754949\n",
      "Epochs:  666 ; Loss:  0.0019585327245295048\n",
      "Epochs:  667 ; Loss:  0.001881732139736414\n",
      "Epochs:  668 ; Loss:  0.0018740856321528554\n",
      "Epochs:  669 ; Loss:  0.0018861196003854275\n",
      "Epochs:  670 ; Loss:  0.0019105507526546717\n",
      "Epochs:  671 ; Loss:  0.0018588064704090357\n",
      "Epochs:  672 ; Loss:  0.0018647433025762439\n",
      "Epochs:  673 ; Loss:  0.0018890746869146824\n",
      "Epochs:  674 ; Loss:  0.0018986943177878857\n",
      "Epochs:  675 ; Loss:  0.0019082158105447888\n",
      "Epochs:  676 ; Loss:  0.0018973109545186162\n",
      "Epochs:  677 ; Loss:  0.001856731716543436\n",
      "Epochs:  678 ; Loss:  0.0018411338096484542\n",
      "Epochs:  679 ; Loss:  0.0018851960776373744\n",
      "Epochs:  680 ; Loss:  0.0018904663156718016\n",
      "Epochs:  681 ; Loss:  0.0019073344301432371\n",
      "Epochs:  682 ; Loss:  0.0018671647412702441\n",
      "Epochs:  683 ; Loss:  0.0018477329285815358\n",
      "Epochs:  684 ; Loss:  0.0018192995339632034\n",
      "Epochs:  685 ; Loss:  0.0018370416946709156\n",
      "Epochs:  686 ; Loss:  0.0019006305374205112\n",
      "Epochs:  687 ; Loss:  0.0019116789335384965\n",
      "Epochs:  688 ; Loss:  0.0018408856121823192\n",
      "Epochs:  689 ; Loss:  0.0018421020358800888\n",
      "Epochs:  690 ; Loss:  0.0018331661121919751\n",
      "Epochs:  691 ; Loss:  0.001848860876634717\n",
      "Epochs:  692 ; Loss:  0.0018363224808126688\n",
      "Epochs:  693 ; Loss:  0.001832495559938252\n",
      "Epochs:  694 ; Loss:  0.001864543417468667\n",
      "Epochs:  695 ; Loss:  0.0018866086611524224\n",
      "Epochs:  696 ; Loss:  0.0018362777773290873\n",
      "Epochs:  697 ; Loss:  0.0018319279188290238\n",
      "Epochs:  698 ; Loss:  0.0018259783973917365\n",
      "Epochs:  699 ; Loss:  0.0018187882378697395\n",
      "Epochs:  700 ; Loss:  0.0018153461860492826\n",
      "Epochs:  701 ; Loss:  0.0018368879100307822\n",
      "Epochs:  702 ; Loss:  0.0018533378606662154\n",
      "Epochs:  703 ; Loss:  0.0017988926265388727\n",
      "Epochs:  704 ; Loss:  0.0017899152589961886\n",
      "Epochs:  705 ; Loss:  0.0018140168394893408\n",
      "Epochs:  706 ; Loss:  0.001861462602391839\n",
      "Epochs:  707 ; Loss:  0.001862959237769246\n",
      "Epochs:  708 ; Loss:  0.0018413725774735212\n",
      "Epochs:  709 ; Loss:  0.0018029912607744336\n",
      "Epochs:  710 ; Loss:  0.0017919724341481924\n",
      "Epochs:  711 ; Loss:  0.001817232696339488\n",
      "Epochs:  712 ; Loss:  0.0017849677242338657\n",
      "Epochs:  713 ; Loss:  0.0017686150968074799\n",
      "Epochs:  714 ; Loss:  0.001777459867298603\n",
      "Epochs:  715 ; Loss:  0.0017853380413725972\n",
      "Epochs:  716 ; Loss:  0.0018149230163544416\n",
      "Epochs:  717 ; Loss:  0.001835617353208363\n",
      "Epochs:  718 ; Loss:  0.001827041618525982\n",
      "Epochs:  719 ; Loss:  0.0018190215341746807\n",
      "Epochs:  720 ; Loss:  0.0017916248179972172\n",
      "Epochs:  721 ; Loss:  0.001759673235937953\n",
      "Epochs:  722 ; Loss:  0.0017463701078668237\n",
      "Epochs:  723 ; Loss:  0.001747515401802957\n",
      "Epochs:  724 ; Loss:  0.001795980497263372\n",
      "Epochs:  725 ; Loss:  0.0019296607933938503\n",
      "Epochs:  726 ; Loss:  0.0018169123213738203\n",
      "Epochs:  727 ; Loss:  0.0017668110085651278\n",
      "Epochs:  728 ; Loss:  0.0017347943503409624\n",
      "Epochs:  729 ; Loss:  0.0017263708868995309\n",
      "Epochs:  730 ; Loss:  0.0017281448235735297\n",
      "Epochs:  731 ; Loss:  0.0017588442424312234\n",
      "Epochs:  732 ; Loss:  0.001869853469543159\n",
      "Epochs:  733 ; Loss:  0.0018061426235362887\n",
      "Epochs:  734 ; Loss:  0.0017594050150364637\n",
      "Epochs:  735 ; Loss:  0.0017449897713959217\n",
      "Epochs:  736 ; Loss:  0.001720596570521593\n",
      "Epochs:  737 ; Loss:  0.0017242488684132695\n",
      "Epochs:  738 ; Loss:  0.001730570336803794\n",
      "Epochs:  739 ; Loss:  0.0018321055686101317\n",
      "Epochs:  740 ; Loss:  0.0018831841880455613\n",
      "Epochs:  741 ; Loss:  0.0017635496333241463\n",
      "Epochs:  742 ; Loss:  0.0017333334544673562\n",
      "Epochs:  743 ; Loss:  0.001717353006824851\n",
      "Epochs:  744 ; Loss:  0.0017293323762714863\n",
      "Epochs:  745 ; Loss:  0.0017402017256245017\n",
      "Epochs:  746 ; Loss:  0.0017221627058461308\n",
      "Epochs:  747 ; Loss:  0.0017175407847389579\n",
      "Epochs:  748 ; Loss:  0.0017311935080215335\n",
      "Epochs:  749 ; Loss:  0.001749058603309095\n",
      "Epochs:  750 ; Loss:  0.0017337622120976448\n",
      "Epochs:  751 ; Loss:  0.0017789133125916123\n",
      "Epochs:  752 ; Loss:  0.0017337366007268429\n",
      "Epochs:  753 ; Loss:  0.00171339837834239\n",
      "Epochs:  754 ; Loss:  0.0017419804353266954\n",
      "Epochs:  755 ; Loss:  0.001742602325975895\n",
      "Epochs:  756 ; Loss:  0.0017065610736608505\n",
      "Epochs:  757 ; Loss:  0.0017248183721676469\n",
      "Epochs:  758 ; Loss:  0.0017387367552146316\n",
      "Epochs:  759 ; Loss:  0.0017268433002755046\n",
      "Epochs:  760 ; Loss:  0.0017410173313692212\n",
      "Epochs:  761 ; Loss:  0.0017320147017017007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:  762 ; Loss:  0.0017172145890071988\n",
      "Epochs:  763 ; Loss:  0.0017144352896139026\n",
      "Epochs:  764 ; Loss:  0.0017078007804229856\n",
      "Epochs:  765 ; Loss:  0.001729407231323421\n",
      "Epochs:  766 ; Loss:  0.0017530337208881974\n",
      "Epochs:  767 ; Loss:  0.0017191184451803565\n",
      "Epochs:  768 ; Loss:  0.001711932709440589\n",
      "Epochs:  769 ; Loss:  0.0016846531070768833\n",
      "Epochs:  770 ; Loss:  0.0016748079797253013\n",
      "Epochs:  771 ; Loss:  0.0017082493286579847\n",
      "Epochs:  772 ; Loss:  0.0017150056082755327\n",
      "Epochs:  773 ; Loss:  0.0017604742897674441\n",
      "Epochs:  774 ; Loss:  0.0017383664380759\n",
      "Epochs:  775 ; Loss:  0.0016769953072071075\n",
      "Epochs:  776 ; Loss:  0.0016554193571209908\n",
      "Epochs:  777 ; Loss:  0.0016540044452995062\n",
      "Epochs:  778 ; Loss:  0.0016783657483756542\n",
      "Epochs:  779 ; Loss:  0.0017641921294853091\n",
      "Epochs:  780 ; Loss:  0.0017960123950615525\n",
      "Epochs:  781 ; Loss:  0.0016803106991574168\n",
      "Epochs:  782 ; Loss:  0.001658282009884715\n",
      "Epochs:  783 ; Loss:  0.0016448772512376308\n",
      "Epochs:  784 ; Loss:  0.0016665095463395119\n",
      "Epochs:  785 ; Loss:  0.001704427762888372\n",
      "Epochs:  786 ; Loss:  0.0016788842622190714\n",
      "Epochs:  787 ; Loss:  0.0016723412554711103\n",
      "Epochs:  788 ; Loss:  0.0016829847590997815\n",
      "Epochs:  789 ; Loss:  0.001645780517719686\n",
      "Epochs:  790 ; Loss:  0.0016381611349061131\n",
      "Epochs:  791 ; Loss:  0.0016666962765157223\n",
      "Epochs:  792 ; Loss:  0.0017856042832136154\n",
      "Epochs:  793 ; Loss:  0.0017959235701709986\n",
      "Epochs:  794 ; Loss:  0.0016794127877801657\n",
      "Epochs:  795 ; Loss:  0.0016335505060851574\n",
      "Epochs:  796 ; Loss:  0.0016181296668946743\n",
      "Epochs:  797 ; Loss:  0.0016109001589938998\n",
      "Epochs:  798 ; Loss:  0.0016272444045171142\n",
      "Epochs:  799 ; Loss:  0.001712446566671133\n",
      "Epochs:  800 ; Loss:  0.0017614279640838504\n",
      "Epochs:  801 ; Loss:  0.0017263689078390598\n",
      "Epochs:  802 ; Loss:  0.0016411918913945556\n",
      "Epochs:  803 ; Loss:  0.0016126901609823108\n",
      "Epochs:  804 ; Loss:  0.0016021810006350279\n",
      "Epochs:  805 ; Loss:  0.0016053845174610615\n",
      "Epochs:  806 ; Loss:  0.0016643957933411002\n",
      "Epochs:  807 ; Loss:  0.001766405999660492\n",
      "Epochs:  808 ; Loss:  0.001739877974614501\n",
      "Epochs:  809 ; Loss:  0.0016921450151130557\n",
      "Epochs:  810 ; Loss:  0.0016278664115816355\n",
      "Epochs:  811 ; Loss:  0.001597295398823917\n",
      "Epochs:  812 ; Loss:  0.001589351100847125\n",
      "Epochs:  813 ; Loss:  0.0015856677200645208\n",
      "Epochs:  814 ; Loss:  0.0015996673610061407\n",
      "Epochs:  815 ; Loss:  0.0016970636788755655\n",
      "Epochs:  816 ; Loss:  0.0017896940698847175\n",
      "Epochs:  817 ; Loss:  0.0016720760613679886\n",
      "Epochs:  818 ; Loss:  0.0016416512662544847\n",
      "Epochs:  819 ; Loss:  0.0015974707202985883\n",
      "Epochs:  820 ; Loss:  0.0015831568744033575\n",
      "Epochs:  821 ; Loss:  0.0015751041937619448\n",
      "Epochs:  822 ; Loss:  0.0015734846238046885\n",
      "Epochs:  823 ; Loss:  0.001581960590556264\n",
      "Epochs:  824 ; Loss:  0.0016626279102638364\n",
      "Epochs:  825 ; Loss:  0.0017866746056824923\n",
      "Epochs:  826 ; Loss:  0.0017148187616840005\n",
      "Epochs:  827 ; Loss:  0.001653006998822093\n",
      "Epochs:  828 ; Loss:  0.001590678934007883\n",
      "Epochs:  829 ; Loss:  0.001568897976540029\n",
      "Epochs:  830 ; Loss:  0.0015658021438866854\n",
      "Epochs:  831 ; Loss:  0.00159132806584239\n",
      "Epochs:  832 ; Loss:  0.0017025661654770374\n",
      "Epochs:  833 ; Loss:  0.0016142805106937885\n",
      "Epochs:  834 ; Loss:  0.001583804260008037\n",
      "Epochs:  835 ; Loss:  0.00157114677131176\n",
      "Epochs:  836 ; Loss:  0.0016134945908561349\n",
      "Epochs:  837 ; Loss:  0.0016250195913016796\n",
      "Epochs:  838 ; Loss:  0.0016020466573536396\n",
      "Epochs:  839 ; Loss:  0.0015806762967258692\n",
      "Epochs:  840 ; Loss:  0.0016149893635883927\n",
      "Epochs:  841 ; Loss:  0.001611165702342987\n",
      "Epochs:  842 ; Loss:  0.0015933859394863248\n",
      "Epochs:  843 ; Loss:  0.0015976435970515013\n",
      "Epochs:  844 ; Loss:  0.0016520959325134754\n",
      "Epochs:  845 ; Loss:  0.001599129755049944\n",
      "Epochs:  846 ; Loss:  0.001592466956935823\n",
      "Epochs:  847 ; Loss:  0.0015964542981237173\n",
      "Epochs:  848 ; Loss:  0.00159542472101748\n",
      "Epochs:  849 ; Loss:  0.00157926045358181\n",
      "Epochs:  850 ; Loss:  0.0015821370761841536\n",
      "Epochs:  851 ; Loss:  0.0016155471093952656\n",
      "Epochs:  852 ; Loss:  0.0016093748854473233\n",
      "Epochs:  853 ; Loss:  0.001586889149621129\n",
      "Epochs:  854 ; Loss:  0.001569171086885035\n",
      "Epochs:  855 ; Loss:  0.0015501801390200853\n",
      "Epochs:  856 ; Loss:  0.001613749423995614\n",
      "Epochs:  857 ; Loss:  0.0016493565635755658\n",
      "Epochs:  858 ; Loss:  0.0016090080607682467\n",
      "Epochs:  859 ; Loss:  0.0015634644078090787\n",
      "Epochs:  860 ; Loss:  0.0015349393943324685\n",
      "Epochs:  861 ; Loss:  0.001534599345177412\n",
      "Epochs:  862 ; Loss:  0.001579378149472177\n",
      "Epochs:  863 ; Loss:  0.0016227462328970432\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m#print(\"1 batch\")\u001b[39;00m\n\u001b[0;32m     44\u001b[0m loss_epoch\u001b[38;5;241m.\u001b[39mappend(epochs)\n\u001b[1;32m---> 45\u001b[0m loss_values\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epochs\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpochs: \u001b[39m\u001b[38;5;124m\"\u001b[39m, epochs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m; Loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_epoch=[]\n",
    "loss_values = []\n",
    "loss=1\n",
    "epochs=0\n",
    "\n",
    "print(\"Epochs    Loss\")\n",
    "\n",
    "while(loss>tol):\n",
    "    epochs=epochs+1\n",
    "    scheduler.step()\n",
    "    \n",
    "    for x_batch, y_batch in loader:\n",
    "        # Forward pass\n",
    "        y_pred=model(x_batch)\n",
    "        #x_2d= torch.nn.Sequential(torch.nn.Unflatten(1, (1,80,80)))(x_batch)\n",
    "        # Compute laplacian\n",
    "        y_pred= torch.nn.Sequential(torch.nn.Unflatten(1, (80,80)))(y_pred)\n",
    "        \n",
    "        #with torch.no_grad():\n",
    "            #y_pred=y_predg*1\n",
    "            #print(y_pred.requires_grad)\n",
    "        \n",
    "        #print(y_pred.requires_grad)\n",
    "        #y_pred=y_pred.resize_(100, 80, 80)\n",
    "        #print(y_pred.size())\n",
    "        z=torch.concat([y_pred[:, :, :1], y_pred], axis=2)[:, :, :80]\n",
    "        w=torch.concat([y_pred, y_pred[:, :, 79:]], axis=2)[:, :, 1:]\n",
    "        m=torch.concat([y_pred[:, :1, :], y_pred], axis=1)[:, :80, :]\n",
    "        n=torch.concat([y_pred, y_pred[:, 79:, :]], axis=1)[:, 1:, :]\n",
    "        L_y_pred=(z+w+m+n-4*y_pred)*40 # h=1/80 /2h=>*40\n",
    "        #print(L_y_pred.size())\n",
    "        \n",
    "        loss=torch.nn.functional.mse_loss(L_y_pred, x_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.requires_grad_(True)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Weights\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        #print(\"1 batch\")\n",
    "    \n",
    "    loss_epoch.append(epochs)\n",
    "    loss_values.append(loss.item())\n",
    "    \n",
    "    if epochs%1==0:\n",
    "        print(\"Epochs: \", epochs, \"; Loss: \", loss.item())\n",
    "        \n",
    "    loss=loss.item()\n",
    "\n",
    "print(epochs, \"    \", loss.item())\n",
    "\n",
    "#Plot loss function\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(loss_epoch, loss_values)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc9dbba",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea4f7897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot loss function\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(loss_epoch, loss_values)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.semilogy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d43359",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f441f368",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [3, 1, 3, 3], but got 2-dimensional input of size [1, 6400] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      3\u001b[0m PATH\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_jit_uns_cnn_3.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m traced_net\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m6400\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m traced_net\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m      6\u001b[0m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39msave(traced_net, PATH)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_trace.py:741\u001b[0m, in \u001b[0;36mtrace\u001b[1;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func\n\u001b[0;32m    740\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m--> 741\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrap_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[0;32m    756\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    757\u001b[0m ):\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trace_module(\n\u001b[0;32m    759\u001b[0m         func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m,\n\u001b[0;32m    760\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m: example_inputs},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    767\u001b[0m         _module_class,\n\u001b[0;32m    768\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_trace.py:958\u001b[0m, in \u001b[0;36mtrace_module\u001b[1;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[0;32m    954\u001b[0m     argument_names \u001b[38;5;241m=\u001b[39m get_callable_argument_names(func)\n\u001b[0;32m    956\u001b[0m example_inputs \u001b[38;5;241m=\u001b[39m make_tuple(example_inputs)\n\u001b[1;32m--> 958\u001b[0m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_method_from_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvar_lookup_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43margument_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    967\u001b[0m check_trace_method \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_c\u001b[38;5;241m.\u001b[39m_get_method(method_name)\n\u001b[0;32m    969\u001b[0m \u001b[38;5;66;03m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1090\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1088\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1090\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mConvNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 18\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n\u001b[0;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# flatten all dimensions except the batch dimension\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1090\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1088\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1090\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:446\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:442\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    440\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    441\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [3, 1, 3, 3], but got 2-dimensional input of size [1, 6400] instead"
     ]
    }
   ],
   "source": [
    "device=torch.device('cpu')\n",
    "model=model.to(device)\n",
    "PATH= \"model_jit_uns_cnn_3.pth\"\n",
    "traced_net=torch.jit.trace(model, (torch.randn(1,6400)).to(device))\n",
    "traced_net.to(torch.float64)\n",
    "torch.jit.save(traced_net, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b811b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_net=torch.jit.trace(model, (torch.randn(1, 1, 80,80)).to(device))\n",
    "traced_net.to(torch.float64)\n",
    "torch.jit.save(traced_net, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7807c98a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
